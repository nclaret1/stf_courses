# -*- coding: utf-8 -*-
"""PPI++.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xrr33nbV9W8HosyChQ-ugR1stZrdzGXp
"""

import numpy as np
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import dash
from dash import dcc, html
from dash.dependencies import Input, Output
import io
import base64
import matplotlib.pyplot as plt
from abc import ABC, abstractmethod
from typing import Dict, Optional, Callable

# Constants for Laplace(0,1) moments
VAR_X = 2          # Var(X_j)
VAR_X2 = 20        # Var(X_j²)

# Base Signal class hierarchy
class BaseSignal(ABC):
    """Base class for signal functions."""

    @abstractmethod
    def __call__(self, x: np.ndarray) -> np.ndarray:
        """Compute the signal value for given covariates x."""
        pass

    @abstractmethod
    def get_moments(self, n_samples: int = 300_000,
                    random_state: Optional[int] = None) -> Dict[str, float]:
        """Calculate moments required for variance calculations."""
        pass

    def name(self) -> str:
        """Return a string name for the signal."""
        return self.__class__.__name__


class LinearSignal(BaseSignal):
    """Linear signal: α + βᵀx"""

    def __init__(self, alpha: float = 0.7, beta: Optional[np.ndarray] = None, d: int = 10):
        self.alpha = alpha
        self.d = d
        if beta is None:
            self.beta = np.linspace(1.0, 2.0, d)
        else:
            self.beta = beta

    def __call__(self, x: np.ndarray) -> np.ndarray:
        return self.alpha + x @ self.beta

    def get_moments(self, n_samples: int = 300_000,
                    random_state: Optional[int] = None) -> Dict[str, float]:
        # No quadratic component, so variance of h is 0
        return {"var_h": 0.0}


class PolynomialSignal(BaseSignal):
    """Polynomial signal: α + βᵀx + Σ β̃_j x_j²"""

    def __init__(self, alpha: float = 0.7,
                 beta_lin: Optional[np.ndarray] = None,
                 beta_nl: Optional[np.ndarray] = None,
                 d: int = 10):
        self.alpha = alpha
        self.d = d

        if beta_lin is None:
            self.beta_lin = np.linspace(1.0, 2.0, d)
        else:
            self.beta_lin = beta_lin

        if beta_nl is None:
            self.beta_nl = np.full(d, 0.4)
        else:
            self.beta_nl = beta_nl

    def __call__(self, x: np.ndarray) -> np.ndarray:
        return self.alpha + x @ self.beta_lin + (self.beta_nl * x**2).sum(axis=1)

    def get_moments(self, n_samples: int = 300_000,
                    random_state: Optional[int] = None) -> Dict[str, float]:
        # Analytic variance for h(X) = Σ β̃_j (X_j² - 2)
        var_h = VAR_X2 * np.sum(self.beta_nl**2)
        return {"var_h": var_h}


# Delta functions for signal corruption
class DeltaFunction(ABC):
    """Base class for delta functions that corrupt the base signal."""

    @abstractmethod
    def __call__(self, x: np.ndarray, **kwargs) -> np.ndarray:
        """Compute the delta value for given covariates x."""
        pass

    @abstractmethod
    def get_moments(self, x_samples: np.ndarray) -> Dict[str, float]:
        """Calculate moments using provided samples."""
        pass

    def name(self) -> str:
        """Return a string name for the delta function."""
        return self.__class__.__name__


class NoDelta(DeltaFunction):
    """No corruption (δ(x) = 0)."""

    def __call__(self, x: np.ndarray, **kwargs) -> np.ndarray:
        return np.zeros(len(x))

    def get_moments(self, x_samples: np.ndarray) -> Dict[str, float]:
        return {
            "var_delta": 0.0,
            "mean_delta": 0.0,
            "cov_h_delta": 0.0
        }


class SineDelta(DeltaFunction):
    """Sinusoidal corruption: δ(x) = amp · sin(corr Σ x_j)."""

    def __init__(self, amp: float = 10.0, corr: float = 5.0):
        self.amp = amp
        self.corr = corr

    def __call__(self, x: np.ndarray, **kwargs) -> np.ndarray:
        return self.amp * np.sin(self.corr * x.sum(axis=1))

    def get_moments(self, x_samples: np.ndarray) -> Dict[str, float]:
        delta_vals = self(x_samples)
        var_delta = delta_vals.var(ddof=0)
        mean_delta = delta_vals.mean()
        return {
            "var_delta": var_delta,
            "mean_delta": mean_delta,
            # cov_h_delta will be computed later when we have h
        }


# Treatment effect model
class TreatmentModel:
    """Model for treatment effects."""

    def __init__(self,
                 base_signal: BaseSignal,
                 delta: Optional[DeltaFunction] = None,
                 treatment_func: Optional[Callable] = None):
        """
        Parameters:
        -----------
        base_signal : BaseSignal
            The base signal function
        delta : DeltaFunction, optional
            Corruption function (default is NoDelta)
        treatment_func : Callable, optional
            Custom treatment effect function. If None, uses standard τ(W-p)
        """
        self.base_signal = base_signal
        self.delta = delta if delta is not None else NoDelta()
        self.treatment_func = treatment_func

    def outcome_model(self, x: np.ndarray, w: np.ndarray,
                      p: float = 0.5, tau: float = 1.0,
                      sigma: float = 1.0, rng=None) -> np.ndarray:
        """Generate outcomes Y based on the model."""
        rng = np.random.default_rng(rng)
        f_x = self.base_signal(x) #+ tau * p

        # Add delta corruption if present
        f_tilde_x = f_x + self.delta(x)

        # Apply treatment effect
        if self.treatment_func is None:
            treatment_effect = tau * (w - p)
        else:
            treatment_effect = self.treatment_func(w, p, tau)

        # Add noise
        noise = rng.normal(0.0, sigma, size=len(x))

        return f_tilde_x + treatment_effect + noise

    def generate_data(self, n: int, r: float, p: float = 0.5,
                  tau: float = 1.0, sigma: float = 1.0,
                  random_state: Optional[int] = None) -> Dict[str, Dict[str, np.ndarray]]:
        """Generate labeled and unlabeled datasets."""
        rng = np.random.default_rng(random_state)
        d = self.base_signal.d if hasattr(self.base_signal, 'd') else 10

        # Generate covariates
        X_lab = rng.laplace(0.0, 1.0, size=(n, d))
        X_unl = rng.laplace(0.0, 1.0, size=(int(r * n), d))

        # True conditional means
        f_lab = self.base_signal(X_lab) + tau * p
        f_unl = self.base_signal(X_unl) + tau * p

        # Delta corruption
        delta_lab = self.delta(X_lab)
        delta_unl = self.delta(X_unl)
        f_tilde_lab = f_lab + delta_lab
        f_tilde_unl = f_unl + delta_unl

        # Treatments
        W_lab = rng.binomial(1, p, size=n)
        W_unl = rng.binomial(1, p, size=int(r * n))

        # Outcomes
        Y = self.outcome_model(X_lab, W_lab, p, tau, sigma, rng)

        return {
            "labeled": {
                "X": X_lab,
                "W": W_lab,
                "Y": Y,
                "fX": f_lab,
                "delta": delta_lab,
                "ftilde": f_tilde_lab
            },
            "unlabeled": {
                "X": X_unl,
                "W": W_unl,
                "fX": f_unl,
                "delta": delta_unl,
                "ftilde": f_tilde_unl
            }
        }

    def compute_all_moments(self, n_samples: int = 300_000,
                           random_state: Optional[int] = None) -> Dict[str, float]:
        """Compute all moments needed for variance calculations."""
        rng = np.random.default_rng(random_state)
        d = self.base_signal.d if hasattr(self.base_signal, 'd') else 10

        # Generate large sample
        X_big = rng.laplace(0, 1, size=(n_samples, d))

        # Compute base signal and delta
        f_big = self.base_signal(X_big)
        delta_big = self.delta(X_big)
        f_tilde_big = f_big + delta_big


        # Best linear projection of f_tilde on (1, X)
        mu_X = X_big.mean(axis=0)
        cov_X = np.cov(X_big, rowvar=False)
        cov_Xft = np.cov(X_big.T, f_tilde_big, bias=True)[0:d, d]
        beta_mse = np.linalg.solve(cov_X, cov_Xft)
        alpha_mse = f_tilde_big.mean() - beta_mse @ mu_X

        # h(X) and Δ_f(X)
        h_big = f_tilde_big - (X_big - mu_X) @ beta_mse - alpha_mse
        var_h = h_big.var(ddof=0)

        var_delta = delta_big.var(ddof=0)
        mean_delta = delta_big.mean()
        cov_h_delta = np.cov(h_big, delta_big, bias=True)[0, 1]

        return {
            "var_h": var_h,
            "var_delta": var_delta,
            "mean_delta": mean_delta,
            "cov_h_delta": cov_h_delta
        }


# Estimator functions
def estimate_ppi_pp(data: Dict[str, Dict[str, np.ndarray]],
                   lam: float,
                   p: float = 0.5,
                   ridge: float = 0.0) -> float:
    """PPI++ estimator with correct centring."""
    # Unpack data
    X_L, W_L, Y_L = (data["labeled"][k] for k in ("X", "W", "Y"))
    X_U, W_U = (data["unlabeled"][k] for k in ("X", "W"))

    f_L = data["labeled"]["fX"]
    n, d = X_L.shape
    N = X_U.shape[0]

    # Centre covariates & treatments
    mu_X = X_L.mean(axis=0)
    T_L = W_L - p
    T_U = W_U - p

    Xc_L = X_L - mu_X
    Xc_U = X_U - mu_X

    # Design matrices
    Z_L = np.column_stack((np.ones(n), T_L, Xc_L))
    Z_Lf = Z_L
    Z_U = np.column_stack((np.ones(N), T_U, Xc_U))

    # Normal-equation pieces
    A = (Z_L.T @ Z_L) / n
    b = (Z_L.T @ Y_L) / n

    if lam != 0.0:
        A += lam * (Z_U.T @ Z_U) / N
        A -= lam * (Z_Lf.T @ Z_Lf) / n

        b += lam * (Z_U.T @ data["unlabeled"]["fX"]) / N
        b -= lam * (Z_Lf.T @ f_L) / n

    if ridge > 0.0:
        A += ridge * np.eye(d + 2)

    theta = np.linalg.solve(A, b)
    tau_hat = float(theta[1])

    return tau_hat


def sigma_lambda_closed(lam: float, *, p: float, tau: float, r: float,
                        var_h: float, var_delta: float, cov_h_delta: float,
                        mean_delta: float, sigma2: float) -> float:
    """Closed-form Σ_λ for the general regime."""
    var_W = p * (1 - p)
    one_m = 1 - lam
    gamma = tau * (1 - 2 * p)

    term = (
        sigma2
        + one_m**2 * var_h
        + lam**2 * tau**2 * (1 - 2 * p) ** 2
        + lam**2 * var_delta
        + 2 * (
            one_m * lam * cov_h_delta
            + lam**2 * tau * mean_delta * (1 - 2 * p)
        )
        + (lam**2 / r) * (
            var_h + var_delta + tau**2 * (1 - 2 * p) ** 2
            + 2 * (tau * mean_delta * (1 - 2 * p) - cov_h_delta)
        )
    )
    return term / var_W


def lambda_star(var_h: float, var_delta: float, cov_h_delta: float,
                mean_delta: float, tau: float, p: float, r: float) -> float:
    """Closed-form λ* that minimises the analytic Σ_λ."""
    gamma = tau * (1 - 2 * p)
    kappa = var_h + var_delta + gamma**2

    numerator = var_h - cov_h_delta
    denominator = (
        kappa - 2 * cov_h_delta + 2 * gamma * mean_delta
        + (kappa + 2 * (gamma * mean_delta - cov_h_delta)) / r
    )

    if denominator <= 0:
        return 0.0

    lam_star = numerator / denominator
    return np.clip(lam_star, 0.0, 1.0)

def tau_variance_mc(lam: float, model_: TreatmentModel,
                    reps: int = 500, n: int = 500, r: float = 5.0,
                    p: float = 0.5, tau: float = 1.0, sigma: float = 1.0,
                    random_state: Optional[int] = None) -> float:
    """Empirical Monte-Carlo variance of the treatment effect estimator."""
    rng = np.random.default_rng(random_state)
    vals = []

    for i in range(reps):
        new_seed = rng.integers(0, 2**31, dtype=np.int64)
        data = model_.generate_data(n=n, r=r, p=p, tau=tau, sigma=sigma,
                                  random_state=new_seed)
        vals.append(np.sqrt(n) * (estimate_ppi_pp(data, lam, p=p) - tau))

    vals = np.asarray(vals)
    return vals.var(ddof=1)


# Custom treatment effect functions
def standard_treatment(w: np.ndarray, p: float, tau: float) -> np.ndarray:
    """Standard treatment effect: τ(W-p)."""
    return tau * (w - p)


def nonlinear_treatment(w: np.ndarray, p: float, tau: float) -> np.ndarray:
    """Example of a nonlinear treatment effect: τ·sign(W-p)."""
    return tau * np.sign(w - p)

# Experiment and plotting functions
def plot_variance_vs_lambda(model: TreatmentModel,
                           lambda_grid: np.ndarray,
                           p: float = 0.5,
                           tau: float = 1.0,
                           sigma: float = 1.0,
                           r: float = 5.0,
                           n_mc: int = 100,
                           reps: int = 5000,
                           show_theoretical: bool = True,
                           random_state: Optional[int] = None,
                           ax=None) -> plt.Axes:
    """Plot variance vs lambda for a given model."""
    if ax is None:
        _, ax = plt.subplots(figsize=(10, 6))

    # Get moments for theoretical variance
    moments = model.compute_all_moments(random_state=random_state)

    # Calculate theoretical variance if requested
    if show_theoretical:
        var_theory = [
            sigma_lambda_closed(
                lam, p=p, tau=tau, r=r, sigma2=sigma**2,
                var_h=moments["var_h"],
                var_delta=moments["var_delta"],
                cov_h_delta=moments["cov_h_delta"],
                mean_delta=moments["mean_delta"]
            )
            for lam in lambda_grid
        ]
        ax.plot(lambda_grid, var_theory, marker='o', label='Theoretical')

    # Calculate empirical variance
    var_mc = []
    rng     = np.random.default_rng(random_state)
    for lam in lambda_grid:
      new_seed = rng.integers(0, 2**31, dtype=np.int64)
      var_mc_lam = tau_variance_mc(
              lam, model, reps=reps, n=n_mc, r=r, p=p, tau=tau,
              sigma=sigma, random_state=new_seed
          )
      var_mc.append(var_mc_lam)
    ax.plot(lambda_grid, var_mc, linestyle='--', marker='x', label='Monte Carlo')

    # Calculate and plot optimal lambda if theoretical variance is computed
    if show_theoretical:
        lam_optimal = lambda_star(
            var_h=moments["var_h"],
            var_delta=moments["var_delta"],
            cov_h_delta=moments["cov_h_delta"],
            mean_delta=moments["mean_delta"],
            tau=tau, p=p, r=r
        )
        ax.axvline(x=lam_optimal, color='red', linestyle=':', linewidth=1.5,
                 label=fr'$\lambda^\star={lam_optimal:.2f}$')

    # Format plot
    ax.set_xlabel(r'$\lambda$')
    ax.set_ylabel(r'$\Sigma_\lambda$')
    ax.grid(True, linestyle=':')
    ax.legend()

    return ax


# Examples and demonstrations
def demo_correctly_specified_model():
    """Demo for the correctly specified model."""
    print("Running demonstration for correctly specified model...")

    # Create model
    poly_signal = PolynomialSignal()
    model = TreatmentModel(poly_signal)

    # Set parameters
    lambda_grid = np.linspace(0.0, 1.0, 21)
    p = 0.8
    tau = 1.0
    sigma = 5.0
    r = 5.0

    # Plot variance
    fig, ax = plt.subplots(figsize=(10, 6))
    plot_variance_vs_lambda(
        model, lambda_grid, p=p, tau=tau, sigma=sigma, r=r,
        n_mc=100, reps=1000, random_state=123, ax=ax
    )
    ax.set_title(r"Asymptotic variance of $\hat{\tau}_\lambda$ vs. $\lambda$ (Correctly Specified Model)")
    plt.tight_layout()
    plt.show()


def demo_misspecified_model():
    """Demo for the misspecified model with delta function."""
    print("Running demonstration for misspecified model...")

    # Create model
    poly_signal = PolynomialSignal()
    sine_delta = SineDelta(amp=10.0, corr = 0.4)
    model = TreatmentModel(poly_signal, sine_delta)

    # Set parameters
    lambda_grid = np.linspace(0.0, 1.0, 21)
    p = 0.5
    tau = 1.0
    sigma = 1.0
    r = 5.0

    # Plot variance
    fig, ax = plt.subplots(figsize=(10, 6))
    plot_variance_vs_lambda(
        model, lambda_grid, p=p, tau=tau, sigma=sigma, r=r,
        n_mc=500, reps=1000, random_state=123, ax=ax
    )
    ax.set_title('Asymptotic variance: theory vs empirical (mis-specified model)')
    plt.tight_layout()
    plt.show()


def demo_nonlinear_treatment():
    """Demo for a nonlinear treatment effect model."""
    print("Running demonstration for nonlinear treatment effect...")

    # Create model with nonlinear treatment
    poly_signal = PolynomialSignal()
    model = TreatmentModel(
        poly_signal,
        treatment_func=nonlinear_treatment
    )

    # Set parameters
    lambda_grid = np.linspace(0.0, 1.0, 21)
    p = 0.5
    tau = 1.0
    sigma = 1.0
    r = 5.0

    # Plot variance (only empirical since theory doesn't apply)
    fig, ax = plt.subplots(figsize=(10, 6))
    plot_variance_vs_lambda(
        model, lambda_grid, p=p, tau=tau, sigma=sigma, r=r,
        n_mc=500, reps=1000, show_theoretical=False,
        random_state=123, ax=ax
    )
    ax.set_title(r"Empirical variance of $\hat{\tau}_\lambda$ vs. $\lambda$ (Nonlinear Treatment)")
    plt.tight_layout()
    plt.show()


def compare_models():
    """Compare different models in a single plot."""
    print("Running comparison of different models...")

    # Create models
    linear_model = TreatmentModel(LinearSignal())
    poly_model = TreatmentModel(PolynomialSignal())
    misspec_model = TreatmentModel(PolynomialSignal(), SineDelta(amp=10.0))

    # Set parameters
    lambda_grid = np.linspace(0.0, 1.0, 11)  # Fewer points for speed
    p = 0.5
    tau = 1.0
    sigma = 1.0
    r = 5.0

    # Create figure
    fig, ax = plt.subplots(figsize=(12, 7))

    # Plot each model
    # Linear model
    var_mc_linear = [
        tau_variance_mc(
            lam, linear_model, reps=1000, n=500, r=r, p=p,
            tau=tau, sigma=sigma, random_state=123
        )
        for lam in lambda_grid
    ]
    ax.plot(lambda_grid, var_mc_linear, 'o-', label='Linear Model')

    # Polynomial model
    var_mc_poly = [
        tau_variance_mc(
            lam, poly_model, reps=1000, n=500, r=r, p=p,
            tau=tau, sigma=sigma, random_state=123
        )
        for lam in lambda_grid
    ]
    ax.plot(lambda_grid, var_mc_poly, 's-', label='Polynomial Model')

    # Misspecified model
    var_mc_misspec = [
        tau_variance_mc(
            lam, misspec_model, reps=1000, n=500, r=r, p=p,
            tau=tau, sigma=sigma, random_state=123
        )
        for lam in lambda_grid
    ]
    ax.plot(lambda_grid, var_mc_misspec, 'x-', label='Misspecified Model')

    # Format plot
    ax.set_xlabel(r'$\lambda$')
    ax.set_ylabel(r'Empirical Variance')
    ax.set_title('Comparison of Different Models')
    ax.grid(True, linestyle=':')
    ax.legend()
    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    # Run demonstrations
    demo_correctly_specified_model()
    demo_misspecified_model()
    #demo_nonlinear_treatment()

def create_3d_variance_plot(save_path="variance_3d_plot.html",
                           r=5.0, p=0.5, tau=1.0, sigma=1.0,
                           random_state=123):
    """
    Creates a 3D interactive plot of theoretical variance as a function of correlation (corr) and amplitude (amp)
    for the misspecified case, using the optimal lambda_star for each point, and saves it to an HTML file.

    Parameters:
    -----------
    save_path : str
        Path where the HTML file will be saved
    r : float
        Ratio of unlabeled to labeled data
    p : float
        Probability of treatment
    tau : float
        Treatment effect
    sigma : float
        Noise standard deviation
    random_state : int
        Random seed for reproducibility

    Returns:
    --------
    go.Figure
        The Plotly figure object for further customization if needed
    """
    # Create a grid of values for correlation and amplitude
    corr_range = np.linspace(-0.9, 0.9, 30)
    amp_range = np.linspace(0.1, 5.0, 30)
    corr_grid, amp_grid = np.meshgrid(corr_range, amp_range)

    # Initialize matrices for variance and optimal lambda values
    variance_matrix = np.zeros_like(corr_grid)
    lambda_star_matrix = np.zeros_like(corr_grid)

    # Create the base signal
    poly_signal = PolynomialSignal()

    # Calculate theoretical variance for each combination of corr and amp
    for i, amp in enumerate(amp_range):
        for j, corr in enumerate(corr_range):
            # Create the model with specific amp and corr values
            sine_delta = SineDelta(amp=amp, corr=corr)
            model = TreatmentModel(poly_signal, sine_delta)

            # Compute moments for theoretical variance
            moments = model.compute_all_moments(random_state=random_state)

            # Calculate optimal lambda value
            lam_opt = lambda_star(
                var_h=moments["var_h"],
                var_delta=moments["var_delta"],
                cov_h_delta=moments["cov_h_delta"],
                mean_delta=moments["mean_delta"],
                tau=tau, p=p, r=r
            )
            lambda_star_matrix[i, j] = lam_opt

            # Calculate theoretical variance with optimal lambda
            variance = sigma_lambda_closed(
                lam_opt, p=p, tau=tau, r=r, sigma2=sigma**2,
                var_h=moments["var_h"],
                var_delta=moments["var_delta"],
                cov_h_delta=moments["cov_h_delta"],
                mean_delta=moments["mean_delta"]
            )

            # Store the variance
            variance_matrix[i, j] = variance

    # Create 3D plot with Plotly - variance surface
    fig = go.Figure()

    # Add variance surface
    fig.add_trace(go.Surface(
        x=corr_grid,
        y=amp_grid,
        z=variance_matrix,
        colorscale='Viridis',
        name='Theoretical Variance'
    ))

    # Update layout
    fig.update_layout(
        title='Optimal Theoretical Variance (with λ*) for Misspecified Model',
        scene=dict(
            xaxis_title='Correlation (corr)',
            yaxis_title='Amplitude (amp)',
            zaxis_title='Theoretical Variance'
        ),
        width=900,
        height=700,
        margin=dict(l=65, r=50, b=65, t=90),
    )

    # Add annotations explaining the parameters
    fig.update_layout(
        annotations=[
            dict(
                text=f"Parameters: r={r}, p={p}, τ={tau}, σ={sigma}, using λ*",
                x=0.5,
                y=0.95,
                xref="paper",
                yref="paper",
                showarrow=False,
                font=dict(size=14)
            )
        ]
    )

    # Save the plot to an HTML file
    fig.write_html(save_path)

    # Create a second plot for lambda_star values
    lambda_fig = go.Figure(data=[go.Surface(
        x=corr_grid,
        y=amp_grid,
        z=lambda_star_matrix,
        colorscale='Plasma',
        name='Optimal λ*'
    )])

    # Update layout for lambda plot
    lambda_fig.update_layout(
        title='Optimal λ* Values for Misspecified Model',
        scene=dict(
            xaxis_title='Correlation (corr)',
            yaxis_title='Amplitude (amp)',
            zaxis_title='Optimal λ*'
        ),
        width=900,
        height=700,
        margin=dict(l=65, r=50, b=65, t=90),
    )

    # Save the lambda plot to an HTML file
    lambda_fig.write_html(save_path.replace('.html', '_lambda_star.html'))

    print(f"3D variance plot saved to {save_path}")
    print(f"3D lambda_star plot saved to {save_path.replace('.html', '_lambda_star.html')}")

    return fig

# Example usage:
# create_3d_variance_plot("variance_3d_plot.html")

# Or with custom parameters
create_3d_variance_plot(
    save_path="custom_variance_plot.html",
    r=5.0,
    p=0.5,
    tau=2.0,
    sigma=1.5
)

class BaseSignalW(ABC):
    """Base class for signals that depend on both X and W."""
    @abstractmethod
    def __call__(self, x: np.ndarray, w: np.ndarray) -> np.ndarray:
        pass

    def name(self) -> str:
        return self.__class__.__name__


class InteractionSignal(BaseSignalW):
    """
    Example:  α
            + βᵀ X
            + γᵀ (X ◦ W)    element‑wise interaction
            + δ W           (main effect of W)
            + η W²
    """

    def __init__(self,
                 alpha: float = 0.5,
                 beta_x: Optional[np.ndarray] = None,
                 beta_int: Optional[np.ndarray] = None,
                 delta_w: float = 1.0,
                 eta_w2: float = 0.2,
                 d: int = 10):
        self.alpha = alpha
        self.d = d
        self.beta_x   = beta_x  if beta_x  is not None else np.linspace(1.0, 2.0, d)
        self.beta_int = beta_int if beta_int is not None else np.full(d, 0.3)
        self.delta_w  = delta_w
        self.eta_w2   = eta_w2

    def __call__(self, x: np.ndarray, w: np.ndarray) -> np.ndarray:
        # ensure w is (n,1) for broadcasting
        wcol = w.reshape(-1, 1)
        return (
            self.alpha
            + x @ self.beta_x
            + (x * wcol) @ self.beta_int      # interaction term
            + self.delta_w * w
            + self.eta_w2 * w**2
        )

class GeneralTreatmentModel:
    """Simulate data when Y = f̃(X,W) + ξ  with arbitrary f̃."""

    def __init__(self,
                 signal: BaseSignalW,
                 noise_sd: float = 1.0):
        self.signal  = signal
        self.noise_sd = noise_sd

    def generate_data(self, n: int,
                      p: float = 0.5,
                      random_state: Optional[int] = None) -> Dict[str, np.ndarray]:
        rng = np.random.default_rng(random_state)
        d   = self.signal.d if hasattr(self.signal, "d") else 10

        # covariates and treatment
        X = rng.laplace(0.0, 1.0, size=(n, d))
        W = rng.binomial(1, p, size=n)

        # outcome
        f_tilde = self.signal(X, W)
        Y = f_tilde + rng.normal(0.0, self.noise_sd, size=n)

        return dict(
            X = X,
            W = W,
            Y = Y,
            f_tilde = f_tilde
        )

# ─────────────────────────────────────────────────────────────────────────────
#  ➜  1.  EXTRA MOMENTS for  p = 1/2  (general h, Δ_f)
# ─────────────────────────────────────────────────────────────────────────────
def extra_moments_general(model: TreatmentModel,
                          n_samples: int = 300_000,
                          random_state: Optional[int] = None
                          ) -> Dict[str, float]:
    """
    Return all moments that appear in the general Σ_λ formula
    when p = 1/2.
    """
    rng = np.random.default_rng(random_state)
    d   = model.base_signal.d if hasattr(model.base_signal, "d") else 10

    # Draw a big sample of X  and independent W  ~ Bernoulli(1/2)
    X_big = rng.laplace(0.0, 1.0, size=(n_samples, d))
    W_big = rng.binomial(1, 0.5, size=n_samples)          # p = 1/2
    Wc    = W_big - 0.5

    # Components
    f_X       = model.base_signal(X_big)
    delta_XW  = model.delta(X_big)
    g_XW      = f_X + delta_XW - model.treatment_func(W_big, 0.5, tau=1.0) \
                  if model.treatment_func is not None else f_X + delta_XW
    h_XW      = g_XW - (X_big - X_big.mean(axis=0)) @ np.linalg.solve(
                    np.cov(X_big, rowvar=False),
                    np.cov(X_big.T, g_XW, bias=True)[0:d, d]) \
                - g_XW.mean()                             # mean‐zero part
    hf_X      = f_X - (X_big - X_big.mean(axis=0)) @ np.linalg.solve(
                    np.cov(X_big, rowvar=False),
                    np.cov(X_big.T, f_X, bias=True)[0:d, d]) \
                - f_X.mean()
    Delta_f   = h_XW - hf_X

    # Moments
    var_hW   = np.var(h_XW * Wc, ddof=0)
    var_DW   = np.var(Delta_f * Wc, ddof=0)
    cov_h_DW = np.cov(h_XW * Wc, Delta_f * Wc, bias=True)[0, 1]
    var_hf   = np.var(hf_X, ddof=0)
    cov_h_hf = np.mean(h_XW * hf_X)                       # E[h·h^f]

    return dict(
        var_hW   = var_hW,
        var_DW   = var_DW,
        cov_h_DW = cov_h_DW,
        var_hf   = var_hf,
        cov_h_hf = cov_h_hf
    )


# ─────────────────────────────────────────────────────────────────────────────
#  ➜  2.  ANALYTIC VARIANCE  Σ_λ  in the GENERAL regime  (p = 1/2)
# ─────────────────────────────────────────────────────────────────────────────
def sigma_lambda_general(
    lam: float, *, r: float,
    sigma2: float,
    # moments from extra_moments_general(...)
    var_hW: float,
    var_DW: float,
    cov_h_DW: float,
    var_hf: float,
    cov_h_hf: float
) -> float:
    """
    Analytic Σ_λ for p = 1/2, general h and Δ_f.
    """
    varW = 0.25                                 # Var(W) at p = 1/2
    one_m = 1 - lam

    term1 = varW * sigma2
    term2 = (one_m**2) * var_hW
    term3 = lam**2 * var_DW
    term4 = 2 * one_m * lam * cov_h_DW
    term5 = (lam**2 / r) * (var_hf)
    term6 = (- lam * cov_h_hf) / (2 * varW)     # matches your derivation

    return (term1 + term2 + term3 + term4) / varW**2 + term5 + term6


def lambda_star_general(*,                     # convenient wrapper
    r: float,
    var_hW: float,
    var_DW: float,
    cov_h_DW: float,
    var_hf: float,
    cov_h_hf: float
) -> float:
    """Closed‑form λ* minimising Σ_λ for p = 1/2 general case."""
    A = (var_DW + var_hW - 2 * cov_h_DW) + var_hf / r
    B = (cov_h_DW - var_hW) + cov_h_hf / (4 * 0.25)
    lam_star = np.clip(B / A, 0.0, 1.0)
    return lam_star

def variance_mc(lam, rng_seed):
    rng = np.random.default_rng(rng_seed)
    vals = []
    for _ in range(reps_mc):
        data_all = gen_model.generate_data(M, p = p,
                                            random_state = int(rng.integers(1e9)))
        # split: first n labelled, rest unlabeled
        X_L, W_L, Y_L = (a[:n] for a in (data_all['X'],
                                          data_all['W'],
                                          data_all['Y']))
        X_U, W_U      = (a[n:] for a in (data_all['X'],
                                          data_all['W']))

        f_all = data_all['f_tilde']
        f_L, f_U = f_all[:n], f_all[n:]

        d = {
            "labeled":   dict(X = X_L, W = W_L, Y = Y_L, fX = f_L),
            "unlabeled": dict(X = X_U, W = W_U,          fX = f_U)
        }
        tau_hat = estimate_ppi_pp(d, lam, p = p)
        vals.append(np.sqrt(n) * (tau_hat - tau))
    return np.var(vals, ddof = 1)



beta_x_custom   = np.array([ 0.5,  1.0,  1.5,  2.0,  2.5,
                             3.0,  3.5,  4.0,  4.5,  5.0])  # length d
beta_int_custom = np.array([ 0.2,  0.2,  0.3,  0.3,  0.4,
                             0.4,  0.5,  0.5,  0.6,  0.6])

# 1 build the signal with those vectors
signal = InteractionSignal(
    alpha    = 0.7,
    beta_x   = beta_x_custom,
    beta_int = beta_int_custom,
    delta_w  = 1.0,
    eta_w2   = 0.3,
    d        = 10
)
print("β_x    =", signal.beta_x)
print("β_int =", signal.beta_int)

# 2.  build the general model  Y = f̃(X,W) + ξ
gen_model = GeneralTreatmentModel(signal, noise_sd = 1.0)

# 3.  analytic moments  (n_samples large for accuracy)
mom = extra_moments_general(gen_model, n_samples = 300_000,
                            random_state = 123)

# 4.  parameters and λ‑grid
n       = 500                 # labelled sample size
r       = 5.0                 # |U| / |L|
M       = int((1 + r) * n)    # total obs each repetition
p       = 0.5                 # treatment prob
tau     = 1.0
sigma2  = 1.0
reps_mc = 1000
lam_grid = np.linspace(0.0, 1.0, 21)

# 5.  theoretical Σ_λ
sigma_theory = [
    sigma_lambda_general(
        lam, r = r, sigma2 = sigma2, **mom
    )
    for lam in lam_grid
]

# 6.  Monte‑Carlo variance of  √n( ̂τ_λ − τ )
sigma_mc = [variance_mc(lam, rng_seed = 456) for lam in lam_grid]

# 7.  λ*   (general case, p = 1/2)
lam_star = lambda_star_general(r = r, **mom)

# 8.  plot
plt.figure(figsize = (10, 6))
plt.plot(lam_grid, sigma_theory, 'o-', label = 'Theory (general)')
plt.plot(lam_grid, sigma_mc,  'x--', label = 'Monte‑Carlo')
plt.axvline(lam_star, color = 'red', linestyle = ':',
            label = fr"$\lambda^\star = {lam_star:.2f}$")
plt.xlabel(r'$\lambda$')
plt.ylabel(r'$\Sigma_\lambda$')
plt.title('General‑case variance of $\\hat\\tau_\\lambda$ vs $\\lambda$ '
          '(p = 1/2)')
plt.grid(True, linestyle = ':')
plt.legend()
plt.tight_layout()
plt.show()

